{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 1.1: Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Section 0: Package initialisations, environment configuration and function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Non-interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Interactive plotting\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import download_plotlyjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Configure environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Global Variables\n",
    "CURRENT_DIR = '/Users/christophertee/Dropbox/University/MASc/Courses/Winter 2017' + \\\n",
    "'/ECE521 (Inference Algorithms & Machine Learning)/Assignment 2'\n",
    "LOG_DIR = '/Logs'\n",
    "\n",
    "# Activate Plotly Offline for Jupyter\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load two-class notMNIST dataset into training, validation and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load notMNIST dataset\n",
    "'''\n",
    " Training Set: 3500 images\n",
    " Validation Set: 100 images\n",
    " Test Set: 145 images\n",
    " Images are 28 x 28 (normalised) pixels\n",
    "'''\n",
    "with np.load(\"notMNIST.npz\") as data:\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    \n",
    "    # Subsetting data for classes 'C' (2) and 'J' (9)\n",
    "    posClass = 2 # 'C'\n",
    "    negClass = 9 # 'J'\n",
    "    dataIndx = (Target==posClass) + (Target==negClass)\n",
    "    \n",
    "    Data = Data[dataIndx] / 255.\n",
    "    Target = Target[dataIndx].reshape(-1, 1)\n",
    "    \n",
    "    # Converts target labels to 'C' (0) and 'J' (1)\n",
    "    Target[Target==posClass] = 1\n",
    "    Target[Target==negClass] = 0\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(521)\n",
    "    \n",
    "    # Generate and shuffle random index\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    \n",
    "    Data = Data[randIndx]\n",
    "    Target = Target[randIndx]\n",
    "    \n",
    "    # Flatten arrays of dimension m x 28 x 28 into array of dimension m x 784\n",
    "    Data = Data.reshape(Data.shape[0], -1)\n",
    "    \n",
    "    # Standardizing inputs of dataset\n",
    "    Data -= np.mean(Data, axis=0)\n",
    "    Data /= np.std(Data, axis=0)\n",
    "    \n",
    "    # Partition data into training, validation and test datasets\n",
    "    Data, Target = Data[randIndx], Target[randIndx]\n",
    "    trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "    validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "    testData, testTarget = Data[3600:], Target[3600:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create TensorFlow graph for linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildLinearGraph(eta, lambda_):\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('linear_regression'):\n",
    "            # Model inputs\n",
    "            with tf.name_scope('placeholders'):\n",
    "                X = tf.placeholder(tf.float32, shape=[None, None], name='Input')\n",
    "                Y = tf.placeholder(tf.float32, shape=[None, None], name='Target')\n",
    "\n",
    "            # Model parameters\n",
    "            with tf.name_scope('parameters'):\n",
    "                W = tf.get_variable('weights', shape=[784, 1], initializer=tf.truncated_normal_initializer(stddev=0.5))\n",
    "                b = tf.get_variable('biases', shape=[1, 1], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "            with tf.device('/cpu:0'):\n",
    "                # Predicted target\n",
    "                with tf.name_scope('prediction'):\n",
    "                    Yhat = tf.add(tf.matmul(X, W), b, 'pred')\n",
    "\n",
    "                # Metrics\n",
    "                with tf.name_scope('metrics'):\n",
    "                    with tf.name_scope('error'):\n",
    "                        error = tf.add(tf.reduce_mean(tf.nn.l2_loss(tf.subtract(Yhat, Y), name='elem_l2_loss'), \\\n",
    "                                                      name='total_l2_loss'), \\\n",
    "                                       tf.multiply(lambda_ / 2, tf.matmul(tf.transpose(W), W), name='l2_loss'), \\\n",
    "                                       name='total_loss')\n",
    "                    with tf.name_scope('threshold'):\n",
    "                        YhatThres = tf.cast(tf.greater_equal(Yhat, 0.5, name='pred_thres'), tf.float32)\n",
    "                    with tf.name_scope('accuracy'):\n",
    "                        accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Y), tf.int32), name='total_matches'), \\\n",
    "                                              tf.shape(X)[0], \\\n",
    "                                              name='accuracy')\n",
    "\n",
    "        # Optimizer\n",
    "        with tf.device('/cpu:0'):\n",
    "            optimizer = tf.train.AdamOptimizer(eta).minimize(error)\n",
    "    \n",
    "    return W, b, X, Y, YhatThres, error, accuracy, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create TensorFlow graph for logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildLogisticGraph(eta, lambda_, opt):\n",
    "    assert opt in ['GD', 'Adam']\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('logistic_regression'):\n",
    "            # Model inputs\n",
    "            with tf.name_scope('placeholders'):\n",
    "                X = tf.placeholder(tf.float32, shape=[None, None], name='input')\n",
    "                Y = tf.placeholder(tf.float32, shape=[None, None], name='target')\n",
    "\n",
    "            # Model parameters\n",
    "            with tf.name_scope('parameters'):\n",
    "                W = tf.get_variable('weights', shape=[784, 1], initializer=tf.truncated_normal_initializer(stddev=0.5))\n",
    "                b = tf.get_variable('biases', shape=[1, 1], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "            with tf.device('/cpu:0'):\n",
    "                # Predicted target\n",
    "                with tf.name_scope('prediction'):\n",
    "                    Z = tf.add(tf.matmul(X, W), b, 'logits')\n",
    "                    Yhat = tf.sigmoid(Z, name='pred')\n",
    "\n",
    "                # Metrics\n",
    "                with tf.name_scope('metrics'):\n",
    "                    with tf.name_scope('error'):\n",
    "                        error = tf.add(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=Y, name='elem_x_entropy'), \\\n",
    "                                                      name='total_x_entropy'), \\\n",
    "                                       tf.multiply(lambda_ / 2, tf.matmul(tf.transpose(W), W), name='l2_loss'), \\\n",
    "                                       name='total_loss')\n",
    "                    with tf.name_scope('threshold'):\n",
    "                        YhatThres = tf.cast(tf.greater_equal(Yhat, 0.5, name='pred_thres'), tf.float32)\n",
    "                    with tf.name_scope('accuracy'):\n",
    "                        accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Y), tf.int32), name='total_matches'), \\\n",
    "                                              tf.shape(X)[0], \\\n",
    "                                              name='accuracy')\n",
    "\n",
    "        # Optimizer\n",
    "        with tf.device('/cpu:0'):\n",
    "            if opt == 'GD':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(eta).minimize(error)\n",
    "            elif opt == 'Adam':\n",
    "                optimizer = tf.train.AdamOptimizer(eta).minimize(error)\n",
    "    \n",
    "    return W, b, X, Y, YhatThres, error, accuracy, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create TensorFlow graph for multinomial logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildSoftmaxGraph(eta, lambda_):\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('softmax'):\n",
    "            # Model inputs\n",
    "            with tf.name_scope('placeholders'):\n",
    "                X = tf.placeholder(tf.float32, shape=[None, None], name='input')\n",
    "                Y = tf.placeholder(tf.float32, shape=[None, None], name='target')\n",
    "\n",
    "            # Model parameters\n",
    "            with tf.name_scope('parameters'):\n",
    "                W = tf.get_variable('weights', shape=[784, 10], initializer=tf.truncated_normal_initializer(stddev=0.5))\n",
    "                b = tf.get_variable('biases', shape=[1, 10], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Predicted target\n",
    "        with tf.name_scope('prediction'):\n",
    "            Z = tf.add(tf.matmul(X, W), b, 'logits')\n",
    "            Yhat = tf.nn.softmax(Z, name='activation')\n",
    "\n",
    "        # Metrics\n",
    "        with tf.name_scope('metrics'):\n",
    "            with tf.name_scope('error'):\n",
    "                with tf.name_scope('x_entropy'):\n",
    "                    x_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z, \\\n",
    "                                                                                       labels=Y,\\\n",
    "                                                                                       name='elem_x_entropy'), \\\n",
    "                                               name='total_x_entropy')\n",
    "                with tf.name_scope('l2_loss'):\n",
    "                    l2_loss = tf.multiply(lambda_ / 2, \\\n",
    "                                          tf.reduce_sum(tf.matmul(tf.transpose(W), W)), name='l2_loss')\n",
    "                with tf.name_scope('total_loss'):\n",
    "                    error = tf.add(x_entropy, l2_loss, name='total_loss')\n",
    "            with tf.name_scope('threshold'):\n",
    "                YhatThres = tf.cast(tf.argmax(Yhat, axis=1, name='pred_threshold'), tf.float32)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                Ycollapsed = tf.cast(tf.argmax(Y, axis=1), tf.float32, name='target_reverse_one_hot')\n",
    "                accuracy = tf.truediv(tf.reduce_sum(tf.cast(tf.equal(YhatThres, Ycollapsed) \\\n",
    "                                                            , tf.int32), \\\n",
    "                                                    name='total_matches'), \\\n",
    "                                      tf.shape(X)[0], \\\n",
    "                                      name='accuracy')\n",
    "\n",
    "    # Optimizer\n",
    "    with tf.device('/cpu:0'):\n",
    "        optimizer = tf.train.AdamOptimizer(eta).minimize(error)\n",
    "    \n",
    "    return W, b, X, Y, YhatThres, error, accuracy, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Plotly interactive graph generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def etaIGraph(summary, title, filename):\n",
    "    \n",
    "    # Create subplot titles\n",
    "    subplotTitle = ['Cross-Entropy Loss vs. Number of Updates', 'Classification Accuracy vs. Number of Updates']\n",
    "    \n",
    "    # Define subplot figure\n",
    "    figure = tools.make_subplots(rows=2, cols=1, subplot_titles=(subplotTitle))\n",
    "\n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colorList = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    # Create plot for each summary\n",
    "    runs = ['train', 'test']\n",
    "    for i, run in enumerate(runs):\n",
    "        crossEntropyTrace = go.Scatter(\n",
    "            x = range(summary['numIter'] + 1),\n",
    "            y = summary[run + 'Error'],\n",
    "            name = run,\n",
    "            marker = {'color': colorList[i]}\n",
    "        )\n",
    "        accuracyTrace = go.Scatter(\n",
    "            x = range(summary['numIter'] + 1),\n",
    "            y = summary[run + 'Accuracy'],\n",
    "            name = run,\n",
    "            marker = {'color': colorList[i]}\n",
    "        )\n",
    "        figure.append_trace(crossEntropyTrace, 1, 1)\n",
    "        figure.append_trace(accuracyTrace, 2, 1)\n",
    "        \n",
    "    # Update subplot axes titles\n",
    "    figure['layout']['xaxis1'].update(title = 'Number of Updates')\n",
    "    figure['layout']['yaxis1'].update(title = 'Cross-Entropy Loss')\n",
    "    figure['layout']['xaxis2'].update(title = 'Number of Updates')\n",
    "    figure['layout']['yaxis2'].update(title = 'Classification Accuracy, %')\n",
    "    \n",
    "    # Update figure layout\n",
    "    figure['layout'].update(\n",
    "        height = 800,\n",
    "        showlegend = False,\n",
    "        title = title,\n",
    "    )\n",
    "\n",
    "    return py.iplot(figure, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define learning rate tuning function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_ITER = 10000\n",
    "def tuneLearningRate(etaList, model, section_dir, batchSize=500, lambda_=0.01, opt='Adam'):    \n",
    "    # Returns the i-th batch of training data and targets\n",
    "    # Generates a new, reshuffled batch once all previous batches are fed\n",
    "    def getNextTrainingBatch(currentIter, randIdx):\n",
    "        currentBatchNum = currentIter % (trainData.shape[0] / batchSize)\n",
    "        if currentBatchNum == 0:\n",
    "            np.random.shuffle(randIdx)\n",
    "        lowerBoundIdx = currentBatchNum * batchSize\n",
    "        upperBoundIdx = (currentBatchNum + 1) * batchSize \n",
    "        return trainData[randIdx[lowerBoundIdx:upperBoundIdx]], trainTarget[randIdx[lowerBoundIdx:upperBoundIdx]]\n",
    "    \n",
    "    # Generate updated plots for training and validation MSE\n",
    "    def plotErrGraph(errList, param):\n",
    "        label = '$\\eta$ = ' + str(param)\n",
    "        label_classification = ['train.', 'valid.']\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8,5), dpi=200)\n",
    "        \n",
    "        for i, err in enumerate(errList):\n",
    "            plt.plot(range(len(err)), err, '-', label=label+' '+label_classification[i])\n",
    "        \n",
    "        plt.axis([0, MAX_ITER, 0, np.amax(errList)])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculates the ratio between the n-th average epoch MSE and the (n-1)-th average epoch MSE\n",
    "    def ratioAverageEpochMSE(currentValidErr):\n",
    "        averageN = np.average(currentValidErr[-(np.arange(epochSize - 1) + 1)])\n",
    "        averageNlessOne = np.average(currentValidErr[-(np.arange(epochSize - 1) + epochSize)])\n",
    "        return averageN / averageNlessOne\n",
    "    \n",
    "    # Returns True if the average epoch validation MSE is at least 99.9% of the previous epoch average.\n",
    "    # i.e. Returns True if the average learnings between epoch is less than +0.1%\n",
    "    # Otherwise, returns False\n",
    "    def shouldStopEarly(currentValidErr):\n",
    "        if currentValidErr.shape[0] < 2 * epochSize:\n",
    "            return False\n",
    "        return True if (ratioAverageEpochMSE(currentValidErr) > 0.999) else False\n",
    "    \n",
    "    \n",
    "    # Start of function\n",
    "    summaryList = []\n",
    "    randIdx = np.arange(trainData.shape[0])\n",
    "    epochSize = trainData.shape[0] / batchSize\n",
    "    randIdx = np.arange(trainData.shape[0])\n",
    "    \n",
    "    assert section_dir\n",
    "    chapter_dir = '/Binary Loss/Section ' + section_dir + '/'\n",
    "    current_time = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "    \n",
    "    for eta in etaList:\n",
    "        # Reset graph to prevent duplication of ops and variables\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build new graph\n",
    "        assert model in ['linear', 'logistic', 'softmax']\n",
    "        if model == 'linear':\n",
    "            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildLinearGraph(eta, lambda_)\n",
    "        elif model == 'logistic':\n",
    "            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildLogisticGraph(eta, lambda_, opt)\n",
    "        elif model == 'softmax':\n",
    "            W, b, X, Y, YhatThres, error, accuracy, optimizer = buildSoftmaxGraph(eta, lambda_)\n",
    "        \n",
    "        # Begin session\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "            # Log starting time\n",
    "            startTime = time.time()\n",
    "            \n",
    "            # Create summary writer\n",
    "            writer = tf.summary.FileWriter(CURRENT_DIR + LOG_DIR + chapter_dir + current_time + \\\n",
    "                                           '/Eta-' + str(eta), \\\n",
    "                                           graph=sess.graph)\n",
    "            \n",
    "            # Initialise all TensorFlow variables\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Creates blank training and validation MSE arrays for the Session\n",
    "            currentTrainErr = np.array([])[:, np.newaxis]\n",
    "            currentValidErr = np.array([])[:, np.newaxis]\n",
    "            currentTestErr = np.array([])[:, np.newaxis]\n",
    "            \n",
    "            currentTrainAcc = np.array([])[:, np.newaxis]\n",
    "            currentValidAcc = np.array([])[:, np.newaxis]\n",
    "            currentTestAcc = np.array([])[:, np.newaxis]\n",
    "    \n",
    "            # Runs update\n",
    "            currentIter = 0\n",
    "            while currentIter < MAX_ITER:\n",
    "                inputData, inputTarget = getNextTrainingBatch(currentIter, randIdx)\n",
    "                \n",
    "                _, trainErr, trainAcc = sess.run([optimizer, error, accuracy], feed_dict={X: inputData, Y: inputTarget})\n",
    "                validErr, validAcc = sess.run([error, accuracy], feed_dict={X: validData, Y: validTarget})\n",
    "                testErr, testAcc = sess.run([error, accuracy], feed_dict={X: testData, Y: testTarget})\n",
    "\n",
    "                currentTrainErr = np.append(currentTrainErr, trainErr)\n",
    "                currentValidErr = np.append(currentValidErr, validErr)\n",
    "                currentTestErr = np.append(currentTestErr, testErr)\n",
    "                \n",
    "                currentTrainAcc = np.append(currentTrainAcc, trainAcc)\n",
    "                currentValidAcc = np.append(currentValidAcc, validAcc)\n",
    "                currentTestAcc = np.append(currentTestAcc, testAcc)\n",
    "                \n",
    "                # Update graph of training and validation MSE arrays\n",
    "#                 if (currentIter < 3) or (currentIter % 1000 == 0):\n",
    "#                     plotErrGraph([currentTrainErr, currentValidErr], eta)\n",
    "                \n",
    "                # At every epoch, check for early stopping possibilty. If so, breaks from while loop\n",
    "                if currentIter % epochSize == 0:\n",
    "                    if shouldStopEarly(currentValidErr):\n",
    "                        writer.close()\n",
    "                        break\n",
    "                \n",
    "                currentIter += 1\n",
    "                \n",
    "                if currentIter == MAX_ITER:\n",
    "                    writer.close()\n",
    "            \n",
    "        # Save session results as dictionary and appends to MSEsummaryList\n",
    "        summaryList.append(\n",
    "            {\n",
    "                'eta': eta,\n",
    "                'B': batchSize,\n",
    "                'lambda': lambda_,\n",
    "                'optimizer': opt,\n",
    "                'numIter': currentIter + 1,\n",
    "                'epoch': float(currentIter + 1) / epochSize,\n",
    "                'trainError': currentTrainErr,\n",
    "                'validError': currentValidErr,\n",
    "                'testError': currentTestErr,\n",
    "                'trainAccuracy': currentTrainAcc,\n",
    "                'validAccuracy': currentValidAcc,\n",
    "                'testAccuracy': currentTestAcc\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Print stats when one eta run is done\n",
    "        print 'eta: %7.3f, numIter: %7d, validError: %.3f, testAcc: %.3f duration: %3.1fs' % \\\n",
    "            (summaryList[-1]['eta'], summaryList[-1]['numIter'], summaryList[-1]['validError'][-1], \\\n",
    "             np.mean(summaryList[-1]['testAccuracy'][-epochSize:]), time.time() - startTime)\n",
    "            \n",
    "    return summaryList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section 1: Tuning Learning Rate, $\\eta$, for Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta:   0.001, numIter:    1254, validError: 1.176, testAcc: 0.862 duration: 8.8s\n",
      "eta:   0.010, numIter:     673, validError: 0.805, testAcc: 0.931 duration: 5.0s\n",
      "eta:   0.100, numIter:     260, validError: 0.523, testAcc: 0.986 duration: 2.1s\n",
      "eta:   1.000, numIter:      78, validError: 0.261, testAcc: 0.993 duration: 0.9s\n",
      "eta:  10.000, numIter:      22, validError: 3.298, testAcc: 0.986 duration: 0.5s\n"
     ]
    }
   ],
   "source": [
    "etaList = [0.001, 0.01, 0.1, 1, 10]\n",
    "summary1_1 = tuneLearningRate(etaList, model='logistic', section_dir='1.1', opt='GD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Produce interactive 2x1 subplots for best learning rate, $\\eta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rsolitude/64.embed\" height=\"800px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1_1 = etaIGraph(summary1_1[3], \\\n",
    "                   '$ \\\\text{Graphs of Best Learning Rate } (\\\\eta=' + \\\n",
    "                   str(summary1_1[3]['eta']) + ') \\\\text{ using Gradient Descent optimizer}$',\\\n",
    "                   'A2Q1.1.1_LogisticSGD')\n",
    "fig1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section 2: Tuning Learning Rate, $\\eta$, for Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta:   0.000, numIter:    1716, validError: 1.173, testAcc: 0.883 duration: 11.5s\n",
      "eta:   0.001, numIter:    1471, validError: 0.327, testAcc: 0.966 duration: 10.0s\n",
      "eta:   0.010, numIter:     253, validError: 0.298, testAcc: 0.979 duration: 2.2s\n",
      "eta:   0.100, numIter:      15, validError: 1.397, testAcc: 0.979 duration: 0.8s\n",
      "eta:   1.000, numIter:      43, validError: 2.115, testAcc: 0.979 duration: 0.8s\n"
     ]
    }
   ],
   "source": [
    "etaList = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "summary1_2 = tuneLearningRate(etaList, model='logistic', section_dir='1.2', opt='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Produce interactive 2x1 subplots for best learning rate, $\\eta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rsolitude/62.embed\" height=\"800px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1_2 = etaIGraph(summary1_2[2], \\\n",
    "                   '$ \\\\text{Graphs of Best Learning Rate } (\\\\eta=' + \\\n",
    "                   str(summary1_2[2]['eta']) + ') \\\\text{ using Adam optimizer}$',\\\n",
    "                   'A2Q1.1.2_LogisticAdam')\n",
    "fig1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section 3: Comparing Linear and Logistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "eta:   0.000, numIter:    7393, validError: 197.854, testAcc: 0.628 duration: 43.8s\n",
      "eta:   0.001, numIter:    1065, validError: 395.130, testAcc: 0.552 duration: 6.7s\n",
      "eta:   0.010, numIter:     253, validError: 119.346, testAcc: 0.690 duration: 1.9s\n",
      "eta:   0.100, numIter:     134, validError: 145.433, testAcc: 0.814 duration: 1.1s\n",
      "eta:   1.000, numIter:     183, validError: 159.603, testAcc: 0.759 duration: 1.5s\n",
      "eta:  10.000, numIter:     120, validError: 14397.502, testAcc: 0.766 duration: 1.1s\n",
      "\n",
      "Logistic Regression:\n",
      "eta:   0.000, numIter:     855, validError: 0.271, testAcc: 0.917 duration: 6.2s\n",
      "eta:   0.001, numIter:     414, validError: 0.310, testAcc: 0.959 duration: 3.1s\n",
      "eta:   0.010, numIter:      57, validError: 0.152, testAcc: 0.938 duration: 0.8s\n",
      "eta:   0.100, numIter:      15, validError: 1.188, testAcc: 0.986 duration: 0.6s\n",
      "eta:   1.000, numIter:      15, validError: 4.711, testAcc: 0.972 duration: 0.6s\n",
      "eta:  10.000, numIter:      22, validError: 47.081, testAcc: 0.966 duration: 0.6s\n"
     ]
    }
   ],
   "source": [
    "etaList = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "print 'Linear Regression:'\n",
    "summary1_3_linear = tuneLearningRate(etaList, model='linear', section_dir='1.3Lin', lambda_=0.0)\n",
    "print '\\nLogistic Regression:'\n",
    "summary1_3_logistic = tuneLearningRate(etaList, model='logistic', section_dir='1.3Log', lambda_=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creating summary table for the two models above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model   trainAcc   validAcc    testAcc        eta\n",
      "    Linear      0.654      0.660      0.621      0.010\n",
      "  Logistic      0.972      0.980      0.938      0.010\n"
     ]
    }
   ],
   "source": [
    "# Create summary table\n",
    "print '%10s %10s %10s %10s %10s' % ('Model', 'trainAcc', 'validAcc', 'testAcc', 'eta')\n",
    "print '%10s %10.3f %10.3f %10.3f %10.3f' % ('Linear', summary1_3_linear[2]['trainAccuracy'][-1], summary1_3_linear[2]['validAccuracy'][-1], \\\n",
    "                                        summary1_3_linear[2]['testAccuracy'][-1], summary1_3_linear[2]['eta'])\n",
    "print '%10s %10.3f %10.3f %10.3f %10.3f' % ('Logistic', summary1_3_logistic[4]['trainAccuracy'][-1], summary1_3_logistic[4]['validAccuracy'][-1], \\\n",
    "                                        summary1_3_logistic[2]['testAccuracy'][-1], summary1_3_logistic[2]['eta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generating dummmy cross-entropy and squared-error losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rsolitude/66.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateDummyLosses():\n",
    "    # Generate dummy data\n",
    "    Yhat = np.linspace(0.01,1,100)[:, np.newaxis]\n",
    "    Ydummy = 0.0\n",
    "\n",
    "    # Calculate losses\n",
    "    xEntropy = - Ydummy * np.log(Yhat) - (1 - Ydummy) * np.log(1 - Yhat)\n",
    "    squaredErr = (Yhat - Ydummy)**2\n",
    "    \n",
    "    # Generate plot traces\n",
    "    xEntropyTrace = go.Scatter(\n",
    "        x = Yhat,\n",
    "        y = xEntropy,\n",
    "        name = 'Cross-Entropy'\n",
    "    )\n",
    "    squareErrTrace = go.Scatter(\n",
    "        x = Yhat,\n",
    "        y = squaredErr,\n",
    "        name = 'Squared-Error'\n",
    "    )\n",
    "    \n",
    "    data = go.Data([xEntropyTrace, squareErrTrace])\n",
    "    \n",
    "    # Generate figure layout\n",
    "    layout = go.Layout(\n",
    "        title = '$\\\\text{Comparison of Cross Entropy and Squared Error Losses on Dummy Variable } y = 0$',\n",
    "        xaxis = {'title': '$\\\\hat{y}$'},\n",
    "        yaxis = {'title': 'Loss'}\n",
    "    )\n",
    "    \n",
    "    # Creates and plots figure\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(figure, filename='A2Q1.1.3_LossComparison')\n",
    "\n",
    "fig1_3 = generateDummyLosses()\n",
    "fig1_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 1.2: Multi-Class Classification using Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load notMNIST dataset into training, validation and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load notMNIST dataset\n",
    "'''\n",
    " Training Set: 15,000 images\n",
    " Validation Set: 1,000 images\n",
    " Test Set: 2,724 images\n",
    " Images are 28 x 28 (normalised) pixels\n",
    "'''\n",
    "with np.load(\"notMNIST.npz\") as data:\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(521)\n",
    "    \n",
    "    # Generate and shuffle random index\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    \n",
    "    Data = Data[randIndx]/255.\n",
    "    Target = Target[randIndx]\n",
    "    \n",
    "    # Generates one-hot version of target\n",
    "    oneHot = np.zeros((Target.shape[0], 10))\n",
    "    oneHot[np.arange(Target.shape[0]), Target] = 1\n",
    "    Target = oneHot\n",
    "    \n",
    "    # Flatten arrays of dimension m x 28 x 28 into array of dimension m x 784\n",
    "    Data = Data.reshape(Data.shape[0], -1)\n",
    "    \n",
    "    # Standardizing inputs of dataset\n",
    "    Data -= np.mean(Data, axis=0)\n",
    "    Data /= np.std(Data, axis=0)\n",
    "    \n",
    "    # Partition data into training, validation and test datasets\n",
    "    trainData, trainTarget = Data[:15000], Target[:15000]\n",
    "    validData, validTarget = Data[15000:16000], Target[15000:16000]\n",
    "    testData, testTarget = Data[16000:], Target[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta:   0.000, numIter:    8371, validError: 1.060, testAcc: 0.841 duration: 200.0s\n",
      "eta:   0.001, numIter:    1921, validError: 0.867, testAcc: 0.853 duration: 46.4s\n",
      "eta:   0.010, numIter:     451, validError: 0.807, testAcc: 0.856 duration: 11.4s\n",
      "eta:   0.100, numIter:     121, validError: 2.091, testAcc: 0.873 duration: 3.6s\n",
      "eta:   1.000, numIter:     121, validError: 18.852, testAcc: 0.873 duration: 3.5s\n"
     ]
    }
   ],
   "source": [
    "etaList = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "summary2_3 = tuneLearningRate(etaList, model='softmax', section_dir='2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rsolitude/60.embed\" height=\"800px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig2_3 = etaIGraph(summary2_3[2],\\\n",
    "                   '$ \\\\text{Graphs of Multiclass Classification with Best Learning Rate } (\\\\eta=' + \\\n",
    "                   str(summary2_3[2]['eta']) + ')$',\\\n",
    "                   'A2Q1.2.3_Softmax')\n",
    "fig2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rsolitude/46.embed\" height=\"800px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig2_3 = etaIGraph(summary2_3[2],\\\n",
    "                   '$ \\\\text{Graphs of Multiclass Classification with Best Learning Rate } (\\\\eta=' + \\\n",
    "                   str(summary2_3[2]['eta']) + ')$',\\\n",
    "                   'A2Q2.3_bestEtaGraph')\n",
    "fig2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
