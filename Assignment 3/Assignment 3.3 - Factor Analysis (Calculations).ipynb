{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.3: Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3.1.0: Package initialisations, environment configuration and function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# TensorFlow embedding API library\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Non-interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Interactive plotting\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import download_plotlyjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Global Variables\n",
    "LOG_DIR = './Logs'\n",
    "\n",
    "# Activate Plotly Offline for Jupyter\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Define global variable SEED\n",
    "SEED = 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data2D.npy and data100D.npy into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tinymnist.npz consists of images of '3's and '5's with dimensions (8 x 8)\n",
    "\n",
    "train_data: 700 data points\n",
    "valid_data: 100 data points\n",
    "test_data: 400 data points\n",
    "\"\"\"\n",
    "with np.load (\"./Data/tinymnist.npz\") as data :\n",
    "    # Generate _data\n",
    "    train_data, train_target = data [\"x\"], data[\"y\"]\n",
    "    valid_data, valid_target = data [\"x_valid\"], data [\"y_valid\"]\n",
    "    test_data, test_target = data [\"x_test\"], data [\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results (Optional; when working resuming work session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_3_1_2 = np.load('./Results/FA/3_1_2.npy')\n",
    "# results_3_1_3 = np.load('./Results/FA/3_1_3.npy')\n",
    "# results_3_1_3_x5000 = np.load('./Results/FA/3_1_3_x5000.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Factor Analysis (FA) TensorFlow graph:\n",
    "\n",
    "### Loss function:\n",
    "\n",
    "$$ \\min_{W, \\Psi} - \\log P(\\mathbf{X}) = - \\sum_{n=1}^B \\log \\mathcal{N}(\\mathbf{x}; \\mathbf{\\mu}, \\mathbf{\\Psi} + \\mathbf{WW}^T) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [
     14
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Builds TensorFlow graph for FA\n",
    "\n",
    "Input:\n",
    "    K: number of latent variables\n",
    "    D: dataset dimension\n",
    "    device: CPU or GPU to perform computation-heavy ops\n",
    "Internal variables:\n",
    "    X: input data matrix (N x D)\n",
    "    mu: mean of each input (D x 1)\n",
    "    psi_vector: variance of x_n given s_n for each dimension (D x 1)\n",
    "    W: latent_matrix that projects s_n from K-dimensions to D-dimensions (K x D)\n",
    "    Sigma: Marginal covariance matrix (D x D)\n",
    "'''\n",
    "def build_FA(K, D, device='cpu'):\n",
    "    '''\n",
    "    Helper function to add histogram tag to variables\n",
    "    Input:\n",
    "        var: variable to be tagged with histogram summary\n",
    "    '''\n",
    "    def _add_histogram(vars_):\n",
    "        for var in vars_:\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    \n",
    "    # Define computation device\n",
    "    try:\n",
    "        assert device == 'cpu' or device == 'gpu'\n",
    "    except AssertionError:\n",
    "        print 'Invalid device chosen. Please use \\'cpu\\' or \\'gpu\\''\n",
    "        quit()\n",
    "    device = '/' + device + ':0'\n",
    "    \n",
    "    SEED = 522\n",
    "    \n",
    "    # Fix TF graph seed\n",
    "    tf.set_random_seed(SEED)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Define placeholder\n",
    "        with tf.name_scope('placeholder'):\n",
    "            X = tf.placeholder(tf.float32, shape=[None, D], name='inputs')\n",
    "            \n",
    "        # Define parameters\n",
    "        with tf.variable_scope('generated_parameters'):\n",
    "            phi = tf.get_variable('latent_for_psi', shape=[D, 1], \\\n",
    "                                initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "            W = tf.get_variable('latent_matrix_W', shape=[D, K], \\\n",
    "                                       initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "            mu = tf.get_variable('mean', shape=[D, 1], initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "        \n",
    "    with tf.device(device):\n",
    "        # Calculate gaussian parameters\n",
    "        with tf.name_scope('gaussian_parameters'):\n",
    "            psi_vector = tf.exp(phi, name='variance_vector_psi')\n",
    "            Psi = tf.multiply(tf.eye(tf.shape(psi_vector)[0]), psi_vector, name='Psi_matrix')\n",
    "            Sigma = tf.add(Psi, tf.matmul(W, tf.transpose(W)), name='Sigma')\n",
    "            \n",
    "        # Calculate projection matrix to infer latent variable s\n",
    "        with tf.name_scope('projection_matrix'):\n",
    "            Sigma_s_posterior = tf.matrix_inverse(tf.eye(K) \\\n",
    "                                       + tf.matmul(tf.transpose(W), \\\n",
    "                                                   tf.matmul(tf.matrix_inverse(Psi), W)))\n",
    "            W_proj = tf.matmul(Sigma_s_posterior, tf.matmul(tf.transpose(W), tf.matrix_inverse(Psi)))\n",
    "            \n",
    "        # Calculate loss function\n",
    "        with tf.name_scope('marginal_log_likelihood'):\n",
    "            with tf.name_scope('Mahalanobis_dist'):\n",
    "                # Expand variables for tensor multiplication\n",
    "                X_expanded = tf.expand_dims(X, axis=2)\n",
    "                mu_expanded = tf.expand_dims(mu, axis=0)\n",
    "\n",
    "                # Calculate mahalanobis distance\n",
    "                Sigma_inv_tiled = tf.tile(tf.expand_dims(tf.matrix_inverse(Sigma), axis=0), \\\n",
    "                                          multiples=[tf.shape(X)[0],1,1], \\\n",
    "                                          name='Sigma_inv_tiled')\n",
    "                \n",
    "                dist = tf.reduce_sum(- 1. / 2 * \\\n",
    "                                   tf.matmul(tf.transpose(X_expanded - mu_expanded, perm=[0,2,1]), \\\n",
    "                                             tf.matmul(Sigma_inv_tiled, \\\n",
    "                                                       X_expanded - mu_expanded)), \\\n",
    "                                   name='Mahalanobis_dist')\n",
    "        \n",
    "            # Calculate log gaussian constant\n",
    "            with tf.name_scope('log_gauss_const'):\n",
    "                log_gauss_const = tf.negative(tf.cast(tf.shape(X)[0], tf.float32) * tf.cast(D, tf.float32)\\\n",
    "                                              * tf.log(2. * np.pi) / 2,\\\n",
    "                                              name='log_gauss_const')\n",
    "            \n",
    "            # Calculate log_determinant\n",
    "            with tf.name_scope('log_det'):\n",
    "                log_det = tf.negative(tf.cast(tf.shape(X)[0], tf.float32) \\\n",
    "                                      * tf.reduce_sum(tf.log(tf.diag_part(tf.cholesky(Sigma)))),\\\n",
    "                                      name='log_det')\n",
    "            \n",
    "            # Calculate loss function\n",
    "            loss = tf.negative(tf.add_n([dist, log_gauss_const, log_det]), name='loss')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        # Define optimizer\n",
    "        with tf.name_scope('Adam_optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01, \\\n",
    "                                               beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)\n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Add histogram summaries for variables of interest\n",
    "        _add_histogram([psi_vector, W, Sigma])\n",
    "        \n",
    "        # Merge all summaries\n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "    return X, mu, psi_vector, W, Sigma, W_proj, loss, optimizer, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs FA training algorithm. Tensorboard embedding enabled\n",
    "'''\n",
    "def run_FA(K_list, QUES_DIR, D=64, device='cpu'):    \n",
    "    '''\n",
    "    Embed data for visualization purposes\n",
    "    '''\n",
    "    def embed_data(D, train_writer):\n",
    "        # Define input data\n",
    "        if D == 64:\n",
    "            input_data = train_data\n",
    "            input_data_name = 'tinymnist_training'\n",
    "        elif D == 3:\n",
    "            input_data = toy_data\n",
    "            input_data_name = 'toy_data'\n",
    "        \n",
    "        # Create variable to embed\n",
    "        data_to_embed = tf.Variable(input_data, name=input_data_name, trainable=False, collections=[])\n",
    "\n",
    "        # Define projector configurations\n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        # Add embedding\n",
    "        embedding = config.embeddings.add()\n",
    "        \n",
    "        # Connect tf.Variable to embedding\n",
    "        embedding.tensor_name = data_to_embed.name\n",
    "\n",
    "        # Evaluate tf.Variable\n",
    "        sess.run(data_to_embed.initializer)\n",
    "        \n",
    "        # Create save checkpoint\n",
    "        saver = tf.train.Saver([data_to_embed])\n",
    "        saver.save(sess, SUMMARY_DIR + sub_dir + '/train/model.ckpt', MAX_ITER)\n",
    "\n",
    "        # Write projector_config.pbtxt in LOG_DIR\n",
    "        projector.visualize_embeddings(train_writer, config)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################    \n",
    "    \n",
    "    # Check compatibility of dataset\n",
    "    try:\n",
    "        assert D == 64 or D == 3\n",
    "    except AssertionError:\n",
    "        print 'Incompatible dataset dimension, D. Please use 64 or 3\\\n",
    "            for tinymnist or toy dataset respectively'\n",
    "        quit()\n",
    "    \n",
    "    # Define locally global function\n",
    "    MAX_ITER = 1200 if D == 64 else 5000\n",
    "    CURR_TIME = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "    SUMMARY_DIR = CURRENT_DIR + LOG_DIR + '/FA/' + QUES_DIR + '/' + CURR_TIME\n",
    "    \n",
    "    # Create list to store run results\n",
    "    results = []\n",
    "    \n",
    "    for K in K_list:\n",
    "        # Clear any pre-defined graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build TensorFlow graph\n",
    "        X, mu, psi_vector, W, Sigma, W_proj, loss, optimizer, merged = build_FA(K, D, device)\n",
    "        \n",
    "        # Select appropriate input_data\n",
    "        if D == 64:\n",
    "            input_data = train_data\n",
    "        elif D == 3:\n",
    "            input_data = toy_data\n",
    "\n",
    "        # Create arrays to log losses, psi and W\n",
    "        train_loss = np.array([])[:, np.newaxis]\n",
    "        if D == 64:\n",
    "            valid_loss = np.array([])[:, np.newaxis]\n",
    "            test_loss = np.array([])[:, np.newaxis]\n",
    "        \n",
    "        mean = np.array([])[:, np.newaxis].reshape(0, D)\n",
    "        psi = np.array([])[:, np.newaxis].reshape(0, D)\n",
    "        Ws = np.array([])[:, np.newaxis, np.newaxis].reshape(0, D, K)\n",
    "        Wproj = np.array([])[:, np.newaxis, np.newaxis].reshape(0, K, D)\n",
    "        \n",
    "        # Begin session\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "            # Log start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create sub-directory title\n",
    "            sub_dir = '/K={},D={}'.format(K, D)\n",
    "            \n",
    "            # Create summary writers\n",
    "            train_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/train', graph=sess.graph)\n",
    "            if D == 64:\n",
    "                valid_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/valid')\n",
    "                test_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/test')\n",
    "                \n",
    "            # Initialise all TensorFlow variables\n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            # Define iterator\n",
    "            curr_iter = 0\n",
    "            \n",
    "            # Calculate training (and validation) loss, \n",
    "            # cluster centres and responsibility indices before any training\n",
    "            err, summaries, curr_mu, curr_psi, curr_W, curr_W_proj = \\\n",
    "                sess.run([loss, merged, mu, psi_vector, W, W_proj], feed_dict={X: input_data})\n",
    "            train_loss = np.append(train_loss, err)\n",
    "            train_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Log psi and W\n",
    "            mean = np.append(mean, np.transpose(curr_mu), axis=0)\n",
    "            psi = np.append(psi, np.transpose(curr_psi), axis=0)\n",
    "            Ws = np.append(Ws, curr_W[np.newaxis, :, :], axis=0)\n",
    "            Wproj = np.append(Wproj, curr_W_proj[np.newaxis, :, :], axis=0)\n",
    "            \n",
    "            if D == 64:\n",
    "                # Log validation loss\n",
    "                err, summaries  = sess.run([loss, merged], feed_dict={X:valid_data})\n",
    "                valid_loss = np.append(valid_loss, err)\n",
    "                valid_writer.add_summary(summaries, curr_iter)\n",
    "\n",
    "                # Log test loss\n",
    "                err, summaries  = sess.run([loss, merged], feed_dict={X:test_data})\n",
    "                test_loss = np.append(test_loss, err)\n",
    "                test_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Begin training\n",
    "            while curr_iter < MAX_ITER:                \n",
    "                # Train graph\n",
    "                _, summaries, err = sess.run([optimizer, merged, loss], feed_dict={X:input_data})\n",
    "                \n",
    "                # Add training loss\n",
    "                train_loss = np.append(train_loss, err)\n",
    "                train_writer.add_summary(summaries, curr_iter + 1)\n",
    "\n",
    "                if D == 64:\n",
    "                    # Log validation loss\n",
    "                    summaries, err = sess.run([merged, loss], feed_dict={X:valid_data})\n",
    "                    valid_loss = np.append(valid_loss, err)\n",
    "                    valid_writer.add_summary(summaries, curr_iter)\n",
    "\n",
    "                    # Log test loss\n",
    "                    err, summaries  = sess.run([loss, merged], feed_dict={X:test_data})\n",
    "                    test_loss = np.append(test_loss, err)\n",
    "                    test_writer.add_summary(summaries, curr_iter)\n",
    "                \n",
    "                # Log responsibility indices and cluster centres every 10% of maximum iteration\n",
    "                if ((float(curr_iter) + 1) * 100 / MAX_ITER) % 10 == 0:\n",
    "                    curr_mu, curr_psi, curr_W, curr_W_proj = sess.run([mu, psi_vector, W, W_proj], feed_dict={X:input_data})\n",
    "                    \n",
    "                    mean = np.append(mean, np.transpose(curr_mu), axis=0)\n",
    "                    psi = np.append(psi, np.transpose(curr_psi), axis=0)\n",
    "                    Ws = np.append(Ws, curr_W[np.newaxis, :, :], axis=0)\n",
    "                    Wproj = np.append(Wproj, curr_W_proj[np.newaxis, :, :], axis=0)\n",
    "                \n",
    "                # Post training progress to user, every 100 iterations\n",
    "                if curr_iter % 100 == 99:\n",
    "                    print 'iter: {:3d}, train_loss: {:5.0f}'.format(curr_iter + 1, train_loss[-1])\n",
    "                \n",
    "                curr_iter += 1\n",
    "            \n",
    "            # End of while loop\n",
    "            print 'Max iteration reached'\n",
    "            \n",
    "            # Embed data\n",
    "            embed_data(D, train_writer)\n",
    "            \n",
    "            # Close writers\n",
    "            train_writer.close()\n",
    "            if D == 64:\n",
    "                valid_writer.close()\n",
    "                test_writer.close()\n",
    "\n",
    "            if D == 64:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'valid_loss': valid_loss,\n",
    "                        'test_loss': test_loss,\n",
    "                        'psi': psi,\n",
    "                        'mean': mean,\n",
    "                        'W': Ws,\n",
    "                        'W_proj': Wproj,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            elif D == 3:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'psi': psi,\n",
    "                        'mean': mean,\n",
    "                        'W': Ws,\n",
    "                        'W_proj': Wproj,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            # TODO calculate convergence\n",
    "            print 'K: {:3d}, duration: {:3.1f}s\\n'\\\n",
    "                .format(K, time.time() - start_time)\n",
    "                                                                              \n",
    "    print 'RUN COMPLETED'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 3.3.1.2: FA on $\\textit{tinymnist.npz}$ with validation $(K = 4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss: 31080\n",
      "iter: 200, train_loss: 15126\n",
      "iter: 300, train_loss:  2180\n",
      "iter: 400, train_loss: -4400\n",
      "iter: 500, train_loss: -6865\n",
      "iter: 600, train_loss: -8385\n",
      "iter: 700, train_loss: -8445\n",
      "iter: 800, train_loss: -8453\n",
      "iter: 900, train_loss: -8453\n",
      "iter: 1000, train_loss: -8453\n",
      "iter: 1100, train_loss: -8453\n",
      "iter: 1200, train_loss: -8453\n",
      "Max iteration reached\n",
      "K:   4, duration: 54.8s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_2 = run_FA(K_list=[4], QUES_DIR='/Q3.1.2', device='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Results/FA/3_1_2.npy', results_3_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.041  0.112  0.198  0.301  0.404  0.467  0.472  0.303  0.072  0.14\n",
      "  0.228  0.288  0.355  0.356  0.364  0.254  0.059  0.152  0.196  0.233\n",
      "  0.265  0.26   0.245  0.082  0.047  0.176  0.321  0.44   0.486  0.439\n",
      "  0.258  0.042  0.032  0.08   0.142  0.19   0.224  0.315  0.382  0.142\n",
      "  0.12   0.058  0.034  0.062  0.103  0.256  0.38   0.2    0.32   0.32\n",
      "  0.234  0.241  0.315  0.402  0.327  0.109  0.182  0.448  0.552  0.537\n",
      "  0.42   0.246  0.095  0.014]\n",
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"ebc2dc14-2040-4bb8-b745-6752d42b2b4e\" style=\"height: 900px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ebc2dc14-2040-4bb8-b745-6752d42b2b4e\", [{\"xaxis\": \"x1\", \"yaxis\": \"y1\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[0.009503389708697796, 0.018956471234560013, 0.028494160622358322, 0.0033042742870748043, -0.027307560667395592, -0.036900315433740616, 0.0007790386443957686, 0.05389109253883362], [0.03924292325973511, 0.08906155079603195, 0.12402860075235367, 0.09678211808204651, 0.06435051560401917, 0.03569880500435829, -0.08921360224485397, -0.11879533529281616], [0.06569468975067139, 0.17665821313858032, 0.12954427301883698, 0.02173732966184616, -0.04723944514989853, -0.12015489488840103, -0.19323880970478058, -0.07103250920772552], [0.06466604024171829, 0.20024210214614868, 0.1757550984621048, 0.02676568739116192, -0.1007811576128006, -0.17268148064613342, -0.13701002299785614, -0.019465649500489235], [0.019493762403726578, 0.06115620583295822, 0.06430201977491379, 0.03969727084040642, 0.0495612695813179, 0.018300194293260574, -0.10023122280836105, -0.06365792453289032], [-0.01086067222058773, -0.0019520599162206054, 0.020575707778334618, 0.0483783483505249, 0.043589044362306595, 0.04775996506214142, -0.055431097745895386, -0.08088761568069458], [-0.02504093013703823, -0.0045738182961940765, 0.03894278407096863, 0.04830271750688553, 0.02693195641040802, -0.018558718264102936, -0.0878608301281929, -0.03707023337483406], [-0.006633442360907793, -0.00030911742942407727, -0.01562444120645523, -0.046518269926309586, -0.06521414965391159, -0.06136789172887802, -0.02433089353144169, -0.0008842436945997179]], \"type\": \"heatmap\"}, {\"xaxis\": \"x2\", \"yaxis\": \"y2\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.039632298052310944, -0.1070239245891571, -0.15657931566238403, -0.1658952832221985, -0.11839203536510468, -0.0479067787528038, 0.04635168984532356, 0.10734718292951584], [-0.06593979150056839, -0.11455734074115753, -0.06489990651607513, -0.0029779491014778614, 0.023344874382019043, -0.028584424406290054, -0.07261114567518234, -0.024871384724974632], [-0.039208654314279556, -0.04222621023654938, 0.05158979445695877, 0.03388053551316261, -0.0442962646484375, -0.09501081705093384, -0.09585312008857727, -0.025762513279914856], [-0.020111482590436935, -0.013174883089959621, -0.01835850067436695, -0.10895956307649612, -0.14439290761947632, -0.14850111305713654, -0.09004170447587967, -0.02183086983859539], [-0.003589804982766509, 0.008969377726316452, 0.02859371341764927, 0.022115839645266533, -0.00966435857117176, -0.07544021308422089, -0.14193323254585266, -0.08701201528310776], [0.03055560030043125, -0.003341941861435771, 0.021382873877882957, 0.06916696578264236, 0.04985411837697029, -0.055400215089321136, -0.15966209769248962, -0.10262926667928696], [0.044582609087228775, -0.02251587249338627, 0.011217847466468811, 0.021843496710062027, -0.07814283668994904, -0.18808166682720184, -0.12522965669631958, -0.031608905643224716], [0.035683900117874146, 0.04415072873234749, -0.026491491124033928, -0.11716648191213608, -0.12480176985263824, -0.05264386534690857, -0.008492711000144482, -0.0006733617628924549]], \"type\": \"heatmap\"}, {\"xaxis\": \"x3\", \"yaxis\": \"y3\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.055048566311597824, -0.13608568906784058, -0.16333502531051636, -0.13275159895420074, -0.055022772401571274, 0.054314158856868744, 0.13047638535499573, 0.11852477490901947], [-0.08383773267269135, -0.07964503020048141, -0.006743143778294325, 0.0508718267083168, 0.053359389305114746, 0.02453630417585373, 0.0510810911655426, 0.08240795135498047], [-0.037582360208034515, -0.012949228286743164, 0.03370286151766777, 0.04931575059890747, 0.0033312432933598757, 0.004417045507580042, 0.024505577981472015, 0.01508636400103569], [-0.0056097544729709625, -0.016850747168064117, 0.0037094620056450367, 0.015027385205030441, 0.02254788763821125, 0.012998688034713268, 0.00019244971917942166, 0.0016397854778915644], [0.001578446477651596, -0.007998280227184296, -0.0037757945246994495, 0.004428769927471876, 0.019411712884902954, 0.05498592182993889, -0.01418828871101141, -0.06187257543206215], [0.03518979623913765, 7.083264063112438e-05, -0.011078390292823315, -0.009853923693299294, 0.026405498385429382, 0.14144673943519592, 0.06126341223716736, -0.17440783977508545], [0.10869098454713821, 0.0526665635406971, -0.02807239070534706, 0.01035935990512371, 0.1347566843032837, 0.181032195687294, -0.09326209872961044, -0.1659625768661499], [0.040497321635484695, 0.10945871472358704, 0.11572812497615814, 0.05840904265642166, -0.046742696315050125, -0.1649322360754013, -0.14615973830223083, -0.033126913011074066]], \"type\": \"heatmap\"}, {\"xaxis\": \"x4\", \"yaxis\": \"y4\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.007599130272865295, -0.028084300458431244, -0.032700322568416595, -0.02732231840491295, -0.015334652736783028, 0.00871881004422903, 0.036385294049978256, 0.04145347326993942], [-0.001214412273839116, 0.015095801092684269, 0.03648267686367035, 0.04154257848858833, 0.02704894170165062, -0.010349499993026257, -0.05416840687394142, -0.0524585135281086], [0.01059259194880724, 0.03944600373506546, 0.0389455147087574, 0.04627911001443863, 0.01588825136423111, -0.044152550399303436, -0.07752817869186401, -0.0233011431992054], [0.01902780309319496, 0.02257184498012066, 0.02393951267004013, 0.020785175263881683, -0.009830901399254799, -0.004860756453126669, -0.011202596127986908, -0.016047434881329536], [0.016311442479491234, -0.00048776331823319197, -0.013536831364035606, -0.017930665984749794, 0.016061684116721153, 0.03972826153039932, 0.04502284899353981, -0.001035708119161427], [0.034540772438049316, 0.014235042035579681, -0.015500952489674091, -0.045983538031578064, -0.07221144437789917, -0.04141709953546524, 0.10783056914806366, 0.04447844251990318], [0.02321275882422924, 0.08318164199590683, -0.009705297648906708, -0.11930913478136063, -0.11768461763858795, 0.07906881719827652, 0.1837037205696106, 0.04085684195160866], [-0.09387966990470886, -0.1026863306760788, 0.005001502577215433, 0.13178709149360657, 0.22023990750312805, 0.1716541200876236, 0.053255289793014526, 0.0013598278164863586]], \"type\": \"heatmap\"}], {\"showlegend\": false, \"yaxis1\": {\"domain\": [0.625, 1.0], \"showticklabels\": false, \"anchor\": \"x1\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"xaxis4\": {\"domain\": [0.55, 1.0], \"showticklabels\": false, \"anchor\": \"y4\", \"ticks\": \"\"}, \"xaxis3\": {\"domain\": [0.0, 0.45], \"showticklabels\": false, \"anchor\": \"y3\", \"ticks\": \"\"}, \"xaxis2\": {\"domain\": [0.55, 1.0], \"showticklabels\": false, \"anchor\": \"y2\", \"ticks\": \"\"}, \"xaxis1\": {\"domain\": [0.0, 0.45], \"showticklabels\": false, \"anchor\": \"y1\", \"ticks\": \"\"}, \"height\": 900, \"width\": 800, \"yaxis2\": {\"domain\": [0.625, 1.0], \"showticklabels\": false, \"anchor\": \"x2\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"yaxis3\": {\"domain\": [0.0, 0.375], \"showticklabels\": false, \"anchor\": \"x3\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"yaxis4\": {\"domain\": [0.0, 0.375], \"showticklabels\": false, \"anchor\": \"x4\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"title\": \"Weights Visualisation of Latent Matrix, W\", \"annotations\": [{\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 1\", \"y\": 1.0, \"x\": 0.225, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 2\", \"y\": 1.0, \"x\": 0.775, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 3\", \"y\": 0.375, \"x\": 0.225, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 4\", \"y\": 0.375, \"x\": 0.775, \"font\": {\"size\": 16}, \"showarrow\": false}]}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Creates 2x2 subplot of weights\n",
    "Each subplot consists of an 8x8 heatmap for the weight of each latent variable\n",
    "Input:\n",
    "    weights: final trained weights (D x K)\n",
    "'''\n",
    "def visualise_weights(weights):\n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "    \n",
    "    # Define empty figure\n",
    "    figure = tools.make_subplots(rows=2, cols=2, subplot_titles=('Weight 1', 'Weight 2', 'Weight 3', 'Weight 4'))\n",
    "    \n",
    "    # Define subplot traces\n",
    "    for i, weight in enumerate(np.transpose(weights)):\n",
    "        trace = go.Heatmap(\n",
    "            z = np.reshape(weight, (8,8)), # TODO Reverse order\n",
    "            colorscale = [[0, '#000000'], [1, '#FFFFFF']],\n",
    "            showscale = False\n",
    "        )\n",
    "        figure.append_trace(trace, i / 2 + 1, i % 2 + 1)\n",
    "        \n",
    "        figure['layout']['xaxis{}'.format(i + 1)].update(showticklabels = False, ticks = '')\n",
    "        figure['layout']['yaxis{}'.format(i + 1)].update(showticklabels = False, ticks = '', autorange='reversed')\n",
    "        \n",
    "    figure['layout'].update(\n",
    "        height = 900,\n",
    "        width = 800,\n",
    "        showlegend = False,\n",
    "        title = 'Weights Visualisation of Latent Matrix, W'\n",
    "    )\n",
    "    \n",
    "    py.iplot(figure, filename='/ECE521: A3/Q3: Factor Analysis/Q1.2_weights_viz', sharing='private')\n",
    "    return pyo.iplot(figure)\n",
    "\n",
    "visualise_weights(results_3_1_2[0]['W'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 3.3.1.3: PCA vs FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate toy dataset of 200, 3D points using the following relations:\n",
    "\n",
    "$$ x_1 = s_1 \\\\ x_2 = s_1 + 0.001 s_2 \\\\ x_3 = 10 s_3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset random seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define variable to store toy data\n",
    "toy_data = np.array([])[np.newaxis, :].reshape(0, 3)\n",
    "\n",
    "for i in range(200):\n",
    "    # Sample s from normal distribution\n",
    "    s = np.random.randn(3,1)\n",
    "    \n",
    "    # Define conversion matrix from s to x\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [1, 0.001, 0], \n",
    "                  [0, 0, 10]])\n",
    "    \n",
    "    toy_data = np.append(toy_data, np.transpose(np.matmul(A, s)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principle component: \n",
      "[[  1.212e-06]\n",
      " [  1.373e-03]\n",
      " [  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Finds the largest principal component by finding the normalised eigenvector corresponding\n",
    "to the largest eigenvalue of the data covariance matrix\n",
    "'''\n",
    "def largest_component_PCA(data):\n",
    "    # Obtain covariance matrix\n",
    "    sigma = np.cov(np.transpose(data))\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    e_value, e_vector = np.linalg.eig(sigma)\n",
    "    \n",
    "    # Return the largest principle component\n",
    "    PC = e_vector[np.argmax(e_value)][:, np.newaxis]\n",
    "    \n",
    "    # Save data\n",
    "    np.save('./Results/FA/3_1_3_PCA_x{}.npy'.format(data.shape[0]), PC)\n",
    "    \n",
    "    return PC\n",
    "\n",
    "print 'Principle component: \\n{}'.format(largest_component_PCA(toy_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss:  4166\n",
      "iter: 200, train_loss:  2799\n",
      "iter: 300, train_loss:  2317\n",
      "iter: 400, train_loss:  2018\n",
      "iter: 500, train_loss:  1816\n",
      "iter: 600, train_loss:  1673\n",
      "iter: 700, train_loss:  1570\n",
      "iter: 800, train_loss:  1496\n",
      "iter: 900, train_loss:  1441\n",
      "iter: 1000, train_loss:  1401\n",
      "iter: 1100, train_loss:  1371\n",
      "iter: 1200, train_loss:  1349\n",
      "iter: 1300, train_loss:  1332\n",
      "iter: 1400, train_loss:  1322\n",
      "iter: 1500, train_loss:  1320\n",
      "iter: 1600, train_loss:  1320\n",
      "iter: 1700, train_loss:  1150\n",
      "iter: 1800, train_loss:  1018\n",
      "iter: 1900, train_loss:   904\n",
      "iter: 2000, train_loss:   795\n",
      "iter: 2100, train_loss:   690\n",
      "iter: 2200, train_loss:   586\n",
      "iter: 2300, train_loss:   484\n",
      "iter: 2400, train_loss:   384\n",
      "iter: 2500, train_loss:   286\n",
      "iter: 2600, train_loss:   218\n",
      "iter: 2700, train_loss:   158\n",
      "iter: 2800, train_loss:   125\n",
      "iter: 2900, train_loss:   228\n",
      "iter: 3000, train_loss:   129\n",
      "iter: 3100, train_loss:   172\n",
      "iter: 3200, train_loss:   353\n",
      "iter: 3300, train_loss:   167\n",
      "iter: 3400, train_loss:   163\n",
      "iter: 3500, train_loss:   116\n",
      "iter: 3600, train_loss:   696\n",
      "iter: 3700, train_loss:   372\n",
      "iter: 3800, train_loss:   129\n",
      "iter: 3900, train_loss:   144\n",
      "iter: 4000, train_loss:   110\n",
      "iter: 4100, train_loss:   107\n",
      "iter: 4200, train_loss:   171\n",
      "iter: 4300, train_loss:   163\n",
      "iter: 4400, train_loss:   283\n",
      "iter: 4500, train_loss:   436\n",
      "iter: 4600, train_loss:   214\n",
      "iter: 4700, train_loss:   150\n",
      "iter: 4800, train_loss:   125\n",
      "iter: 4900, train_loss:   531\n",
      "iter: 5000, train_loss:   672\n",
      "Max iteration reached\n",
      "K:   1, duration: 12.7s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_3 = run_FA(K_list=[1], D=3, QUES_DIR='Q3.1.3', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss: 100993\n",
      "iter: 200, train_loss: 68378\n",
      "iter: 300, train_loss: 56776\n",
      "iter: 400, train_loss: 49570\n",
      "iter: 500, train_loss: 44687\n",
      "iter: 600, train_loss: 41249\n",
      "iter: 700, train_loss: 38779\n",
      "iter: 800, train_loss: 36983\n",
      "iter: 900, train_loss: 35668\n",
      "iter: 1000, train_loss: 34703\n",
      "iter: 1100, train_loss: 33991\n",
      "iter: 1200, train_loss: 33455\n",
      "iter: 1300, train_loss: 33057\n",
      "iter: 1400, train_loss: 32860\n",
      "iter: 1500, train_loss: 32838\n",
      "iter: 1600, train_loss: 32837\n",
      "iter: 1700, train_loss: 32837\n",
      "iter: 1800, train_loss: 32837\n",
      "iter: 1900, train_loss: 32837\n",
      "iter: 2000, train_loss: 32837\n",
      "iter: 2100, train_loss: 32837\n",
      "iter: 2200, train_loss: 28494\n",
      "iter: 2300, train_loss: 25172\n",
      "iter: 2400, train_loss: 22332\n",
      "iter: 2500, train_loss: 19650\n",
      "iter: 2600, train_loss: 17046\n",
      "iter: 2700, train_loss: 14492\n",
      "iter: 2800, train_loss: 11973\n",
      "iter: 2900, train_loss:  9503\n",
      "iter: 3000, train_loss:  7037\n",
      "iter: 3100, train_loss:  5225\n",
      "iter: 3200, train_loss:  4304\n",
      "iter: 3300, train_loss:  2752\n",
      "iter: 3400, train_loss:  3725\n",
      "iter: 3500, train_loss:  3563\n",
      "iter: 3600, train_loss:  3343\n",
      "iter: 3700, train_loss:  4247\n",
      "iter: 3800, train_loss: 15217\n",
      "iter: 3900, train_loss:  4140\n",
      "iter: 4000, train_loss: 11252\n",
      "iter: 4100, train_loss:  7286\n",
      "iter: 4200, train_loss:  5371\n",
      "iter: 4300, train_loss:  3207\n",
      "iter: 4400, train_loss:  3168\n",
      "iter: 4500, train_loss:  3229\n",
      "iter: 4600, train_loss:  3734\n",
      "iter: 4700, train_loss:  8756\n",
      "iter: 4800, train_loss:  4107\n",
      "iter: 4900, train_loss:  3678\n",
      "iter: 5000, train_loss:  3963\n",
      "Max iteration reached\n",
      "K:   1, duration: 31.5s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_3_x5000 = run_FA(K_list=[1], D=3, QUES_DIR='Q3.1.3', device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save FA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./Results/FA/3_1_3.npy', results_3_1_3)\n",
    "np.save('./Results/FA/3_1_3_x5000.npy', results_3_1_3_x5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.06 ]\n",
      " [ 1.065]\n",
      " [-0.1  ]]\n",
      "[[ 1.047]\n",
      " [ 1.043]\n",
      " [-0.032]]\n"
     ]
    }
   ],
   "source": [
    "print results_3_1_3[0]['W'][-1]\n",
    "print results_3_1_3_x5000[0]['W'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.798e-01   4.616e-01  -3.145e-09]]\n",
      "[[  4.771e-01   4.801e-01  -1.198e-09]]\n"
     ]
    }
   ],
   "source": [
    "print results_3_1_3[0]['W_proj'][-1]\n",
    "print results_3_1_3_x5000[0]['W_proj'][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
