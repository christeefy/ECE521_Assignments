{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 3.3: Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section 3.3.1.0: Package initialisations, environment configuration and function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# TensorFlow embedding API library\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Non-interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Interactive plotting\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import download_plotlyjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Configure environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Global Variables\n",
    "CURRENT_DIR = '/Users/christophertee/Dropbox/University/MASc/Courses/Winter 2017' + \\\n",
    "              '/ECE521 (Inference Algorithms & Machine Learning)/Assignment 3'\n",
    "LOG_DIR = '/Logs'\n",
    "\n",
    "# Activate Plotly Offline for Jupyter\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Define global variable SEED\n",
    "SEED = 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load data2D.npy and data100D.npy into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tinymnist.npz consists of images of '3's and '5's with dimensions (8 x 8)\n",
    "\n",
    "train_data: 700 data points\n",
    "valid_data: 100 data points\n",
    "test_data: 400 data points\n",
    "\"\"\"\n",
    "with np.load (\"./Data/tinymnist.npz\") as data :\n",
    "    # Generate _data\n",
    "    train_data, train_target = data [\"x\"], data[\"y\"]\n",
    "    valid_data, valid_target = data [\"x_valid\"], data [\"y_valid\"]\n",
    "    test_data, test_target = data [\"x_test\"], data [\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load results (Optional; when working resuming work session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# results_3_1_2 = np.load('./Results/FA/3_1_2.npy')\n",
    "# results_3_1_3 = np.load('./Results/FA/3_1_3.npy')\n",
    "# results_3_1_3_x5000 = np.load('./Results/FA/3_1_3_x5000.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create Factor Analysis (FA) TensorFlow graph:\n",
    "\n",
    "### Loss function:\n",
    "\n",
    "$$ \\min_{W, \\Psi} - \\log P(\\mathbf{X}) = - \\sum_{n=1}^B \\log \\mathcal{N}(\\mathbf{x}; \\mathbf{\\mu}, \\mathbf{\\Psi} + \\mathbf{WW}^T) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     14
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Builds TensorFlow graph for FA\n",
    "\n",
    "Input:\n",
    "    K: number of latent variables\n",
    "    D: dataset dimension\n",
    "    device: CPU or GPU to perform computation-heavy ops\n",
    "Internal variables:\n",
    "    X: input data matrix (N x D)\n",
    "    mu: mean of each input (D x 1)\n",
    "    psi_vector: variance of x_n given s_n for each dimension (D x 1)\n",
    "    W: latent_matrix that projects s_n from K-dimensions to D-dimensions (K x D)\n",
    "    Sigma: Marginal covariance matrix (D x D)\n",
    "'''\n",
    "def build_FA(K, D, device='cpu'):\n",
    "    '''\n",
    "    Helper function to add histogram tag to variables\n",
    "    Input:\n",
    "        var: variable to be tagged with histogram summary\n",
    "    '''\n",
    "    def _add_histogram(vars_):\n",
    "        for var in vars_:\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    \n",
    "    # Define computation device\n",
    "    try:\n",
    "        assert device == 'cpu' or device == 'gpu'\n",
    "    except AssertionError:\n",
    "        print 'Invalid device chosen. Please use \\'cpu\\' or \\'gpu\\''\n",
    "        quit()\n",
    "    device = '/' + device + ':0'\n",
    "    \n",
    "    # Fix TF graph seed\n",
    "    tf.set_random_seed(SEED)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Define placeholder\n",
    "        with tf.name_scope('placeholder'):\n",
    "            X = tf.placeholder(tf.float32, shape=[None, D], name='inputs')\n",
    "            \n",
    "        # Define parameters\n",
    "        with tf.variable_scope('generated_parameters'):\n",
    "            phi = tf.get_variable('latent_for_psi', shape=[D, 1], \\\n",
    "                                initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "            W = tf.get_variable('latent_matrix_W', shape=[D, K], \\\n",
    "                                       initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "        \n",
    "    with tf.device(device):\n",
    "        # Calculate gaussian parameters\n",
    "        with tf.name_scope('gaussian_parameters'):\n",
    "            mu = tf.transpose(tf.reduce_mean(X, axis=0, keep_dims=True), name='input_mean')\n",
    "            psi_vector = tf.exp(phi, name='variance_vector_psi')\n",
    "            Psi = tf.multiply(tf.eye(tf.shape(psi_vector)[0]), psi_vector, name='Psi_matrix')\n",
    "            Sigma = tf.add(Psi, tf.matmul(W, tf.transpose(W)), name='Sigma')\n",
    "            \n",
    "        # Calculate loss function\n",
    "        with tf.name_scope('marginal_log_likelihood'):\n",
    "            with tf.name_scope('Mahalanobis_dist'):\n",
    "                # Expand variables for tensor multiplication\n",
    "                X_expanded = tf.expand_dims(X, axis=2)\n",
    "                mu_expanded = tf.expand_dims(mu, axis=0)\n",
    "\n",
    "                # Calculate mahalanobis distance\n",
    "                Sigma_inv_tiled = tf.tile(tf.expand_dims(tf.matrix_inverse(Sigma), axis=0), \\\n",
    "                                          multiples=[tf.shape(X)[0],1,1], \\\n",
    "                                          name='Simga_inv_tiled')\n",
    "                \n",
    "                dist = tf.reduce_sum(- 1. / 2 * \\\n",
    "                                   tf.matmul(tf.transpose(X_expanded - mu_expanded, perm=[0,2,1]), \\\n",
    "                                             tf.matmul(Sigma_inv_tiled, \\\n",
    "                                                       X_expanded - mu_expanded)), \\\n",
    "                                   name='Mahalanobis_dist')\n",
    "        \n",
    "            # Calculate log gaussian constant\n",
    "            with tf.name_scope('log_gauss_const'):\n",
    "                log_gauss_const = tf.negative(tf.cast(tf.shape(X)[0], tf.float32) * tf.cast(D, tf.float32)\\\n",
    "                                              * tf.log(2. * np.pi) / 2,\\\n",
    "                                              name='log_gauss_const')\n",
    "            \n",
    "            # Calculate log_determinant\n",
    "            with tf.name_scope('log_det'):\n",
    "                test = tf.cholesky(Sigma)\n",
    "                log_det = tf.negative(tf.cast(tf.shape(X)[0], tf.float32) \\\n",
    "                                      * tf.reduce_sum(tf.log(tf.diag_part(tf.cholesky(Sigma)))),\\\n",
    "                                      name='log_det')\n",
    "            \n",
    "            # Calculate loss function\n",
    "            loss = tf.negative(tf.add_n([dist, log_gauss_const, log_det]), name='loss')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        # Define optimizer\n",
    "        with tf.name_scope('Adam_optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01, \\\n",
    "                                               beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)\n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Add histogram summaries for variables of interest\n",
    "        _add_histogram([psi_vector, W, Sigma])\n",
    "        \n",
    "        # Merge all summaries\n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "    return X, mu, psi_vector, W, Sigma, loss, optimizer, merged, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs FA training algorithm. Tensorboard embedding enabled\n",
    "'''\n",
    "def run_FA(K_list, QUES_DIR, D=64, device='cpu'):    \n",
    "    '''\n",
    "    Embed data for visualization purposes\n",
    "    '''\n",
    "    def embed_data(D, train_writer):\n",
    "        # Define input data\n",
    "        if D == 64:\n",
    "            input_data = train_data\n",
    "            input_data_name = 'tinymnist_training'\n",
    "        elif D == 3:\n",
    "            input_data = toy_data\n",
    "            input_data_name = 'toy_data'\n",
    "        \n",
    "        # Create variable to embed\n",
    "        data_to_embed = tf.Variable(input_data, name=input_data_name, trainable=False, collections=[])\n",
    "\n",
    "        # Define projector configurations\n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        # Add embedding\n",
    "        embedding = config.embeddings.add()\n",
    "        \n",
    "        # Connect tf.Variable to embedding\n",
    "        embedding.tensor_name = data_to_embed.name\n",
    "\n",
    "        # Evaluate tf.Variable\n",
    "        sess.run(data_to_embed.initializer)\n",
    "        \n",
    "        # Create save checkpoint\n",
    "        saver = tf.train.Saver([data_to_embed])\n",
    "        saver.save(sess, SUMMARY_DIR + sub_dir + '/train/model.ckpt', MAX_ITER)\n",
    "\n",
    "        # Write projector_config.pbtxt in LOG_DIR\n",
    "        projector.visualize_embeddings(train_writer, config)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################    \n",
    "    \n",
    "    # Check compatibility of dataset\n",
    "    try:\n",
    "        assert D == 64 or D == 3\n",
    "    except AssertionError:\n",
    "        print 'Incompatible dataset dimension, D. Please use 64 or 3\\\n",
    "            for tinymnist or toy dataset respectively'\n",
    "        quit()\n",
    "    \n",
    "    # Define locally global function\n",
    "    MAX_ITER = 1200 if D == 64 else 5000\n",
    "    CURR_TIME = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "    SUMMARY_DIR = CURRENT_DIR + LOG_DIR + '/FA/' + QUES_DIR + '/' + CURR_TIME\n",
    "    \n",
    "    # Create list to store run results\n",
    "    results = []\n",
    "    \n",
    "    for K in K_list:\n",
    "        # Clear any pre-defined graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build TensorFlow graph\n",
    "        X, mu, psi_vector, W, Sigma, loss, optimizer, merged, test = build_FA(K, D, device)\n",
    "        \n",
    "        # Select appropriate input_data\n",
    "        if D == 64:\n",
    "            input_data = train_data\n",
    "        elif D == 3:\n",
    "            input_data = toy_data\n",
    "\n",
    "        # Create arrays to log losses, psi and W\n",
    "        train_loss = np.array([])[:, np.newaxis]\n",
    "        if D == 64:\n",
    "            valid_loss = np.array([])[:, np.newaxis]\n",
    "            test_loss = np.array([])[:, np.newaxis]\n",
    "        \n",
    "        psi = np.array([])[:, np.newaxis].reshape(0, input_data.shape[1])\n",
    "        Ws = np.array([])[:, np.newaxis, np.newaxis].reshape(0, input_data.shape[1], K)\n",
    "        \n",
    "        # Begin session\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "            # Log start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create sub-directory title\n",
    "            sub_dir = '/K={}'.format(K)\n",
    "            \n",
    "            # Create summary writers\n",
    "            train_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/train', graph=sess.graph)\n",
    "            if D == 64:\n",
    "                valid_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/valid')\n",
    "                test_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/test')\n",
    "                \n",
    "            # Initialise all TensorFlow variables\n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            # Define iterator\n",
    "            curr_iter = 0\n",
    "            \n",
    "            # Calculate training (and validation) loss, \n",
    "            # cluster centres and responsibility indices before any training\n",
    "            err, summaries, curr_psi, curr_W = \\\n",
    "                sess.run([loss, merged, psi_vector, W], feed_dict={X: input_data})\n",
    "            train_loss = np.append(train_loss, err)\n",
    "            train_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Log psi and W\n",
    "            psi = np.append(psi, np.transpose(curr_psi), axis=0)\n",
    "            Ws = np.append(Ws, curr_W[np.newaxis, :, :], axis=0)\n",
    "            \n",
    "            if D == 64:\n",
    "                # Log validation loss\n",
    "                err, summaries  = sess.run([loss, merged], feed_dict={X:valid_data})\n",
    "                valid_loss = np.append(valid_loss, err)\n",
    "                valid_writer.add_summary(summaries, curr_iter)\n",
    "\n",
    "                # Log test loss\n",
    "                err, summaries  = sess.run([loss, merged], feed_dict={X:test_data})\n",
    "                test_loss = np.append(test_loss, err)\n",
    "                test_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Begin training\n",
    "            while curr_iter < MAX_ITER:                \n",
    "                # Train graph\n",
    "                _, summaries, err = sess.run([optimizer, merged, loss], feed_dict={X:input_data})\n",
    "                \n",
    "                # Add training loss\n",
    "                train_loss = np.append(train_loss, err)\n",
    "                train_writer.add_summary(summaries, curr_iter + 1)\n",
    "\n",
    "                if D == 64:\n",
    "                    # Log validation loss\n",
    "                    summaries, err = sess.run([merged, loss], feed_dict={X:valid_data})\n",
    "                    valid_loss = np.append(valid_loss, err)\n",
    "                    valid_writer.add_summary(summaries, curr_iter)\n",
    "\n",
    "                    # Log test loss\n",
    "                    err, summaries  = sess.run([loss, merged], feed_dict={X:test_data})\n",
    "                    test_loss = np.append(test_loss, err)\n",
    "                    test_writer.add_summary(summaries, curr_iter)\n",
    "                \n",
    "                # Log responsibility indices and cluster centres every 10% of maximum iteration\n",
    "                if ((float(curr_iter) + 1) * 100 / MAX_ITER) % 10 == 0:\n",
    "                    curr_psi, curr_W = sess.run([psi_vector, W])\n",
    "                    \n",
    "                    psi = np.append(psi, np.transpose(curr_psi), axis=0)\n",
    "                    Ws = np.append(Ws, curr_W[np.newaxis, :, :], axis=0)\n",
    "                \n",
    "                # Post training progress to user, every 100 iterations\n",
    "                if curr_iter % 100 == 99:\n",
    "                    print 'iter: {:3d}, train_loss: {:5.0f}'.format(curr_iter + 1, train_loss[-1])\n",
    "                \n",
    "                curr_iter += 1\n",
    "            \n",
    "            # End of while loop\n",
    "            print 'Max iteration reached'\n",
    "            \n",
    "            # Embed data\n",
    "            embed_data(D, train_writer)\n",
    "            \n",
    "            # Close writers\n",
    "            train_writer.close()\n",
    "            if D == 64:\n",
    "                valid_writer.close()\n",
    "                test_writer.close()\n",
    "\n",
    "            if D == 64:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'valid_loss': valid_loss,\n",
    "                        'test_loss': test_loss,\n",
    "                        'psi': psi,\n",
    "                        'W': Ws,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            elif D == 3:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'psi': psi,\n",
    "                        'W': Ws,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            # TODO calculate convergence\n",
    "            print 'K: {:3d}, duration: {:3.1f}s\\n'\\\n",
    "                .format(K, time.time() - start_time)\n",
    "                                                                              \n",
    "    print 'RUN COMPLETED'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section 3.3.1.2: FA on $\\textit{tinymnist.npz}$ with validation $(K = 4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss: 24897\n",
      "iter: 200, train_loss:  7658\n",
      "iter: 300, train_loss: -1215\n",
      "iter: 400, train_loss: -5313\n",
      "iter: 500, train_loss: -7154\n",
      "iter: 600, train_loss: -7984\n",
      "iter: 700, train_loss: -8305\n",
      "iter: 800, train_loss: -8430\n",
      "iter: 900, train_loss: -8452\n",
      "iter: 1000, train_loss: -8453\n",
      "iter: 1100, train_loss: -8453\n",
      "iter: 1200, train_loss: -8453\n",
      "Max iteration reached\n",
      "K:   4, duration: 67.4s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_2 = run_FA(K_list=[4], QUES_DIR='/Q3.1.2', device='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('./Results/FA/3_1_2.npy', results_3_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"f94e389a-71e0-44ee-aa44-d831c4ff0a99\" style=\"height: 900px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f94e389a-71e0-44ee-aa44-d831c4ff0a99\", [{\"xaxis\": \"x1\", \"yaxis\": \"y1\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[0.06363347917795181, 0.15866492688655853, 0.1985674947500229, 0.1657456010580063, 0.07329205423593521, -0.05039700120687485, -0.13666485249996185, -0.12678809463977814], [0.10255157947540283, 0.1195659190416336, 0.04580574110150337, -0.028454387560486794, -0.04269697889685631, -0.007564987987279892, -0.04756603389978409, -0.0942465141415596], [0.05832494795322418, 0.059492044150829315, -0.016404835507273674, -0.0536041259765625, -0.006328050047159195, -0.00681864470243454, -0.040057841688394547, -0.023094139993190765], [0.02256304770708084, 0.06404060125350952, 0.03992805629968643, 0.013930550776422024, -0.012139445170760155, -0.019723910838365555, -0.01150626689195633, 0.000526003073900938], [0.002461977768689394, 0.01987278461456299, 0.014864528551697731, 0.0022557389456778765, -0.005875557195395231, -0.034842196851968765, 0.01573634147644043, 0.062346942722797394], [-0.04657416045665741, -0.0012658529449254274, 0.012391244061291218, 0.0102639589458704, -0.01696505770087242, -0.10397157818078995, -0.047511011362075806, 0.16217753291130066], [-0.12014119327068329, -0.05479098856449127, 0.034125521779060364, 0.010182877071201801, -0.0894702821969986, -0.141656294465065, 0.07326164841651917, 0.1496122032403946], [-0.03686550632119179, -0.10113837569952011, -0.10729458928108215, -0.054856494069099426, 0.031091447919607162, 0.13299620151519775, 0.12724637985229492, 0.030492711812257767]], \"type\": \"heatmap\"}, {\"xaxis\": \"x2\", \"yaxis\": \"y2\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.014897105284035206, -0.041447289288043976, -0.07002349942922592, -0.0709591954946518, -0.04367051646113396, -0.023866960778832436, -0.006799313705414534, 0.00426588486880064], [-0.03951966390013695, -0.09986826032400131, -0.11046604067087173, -0.07059253752231598, -0.03776783123612404, -0.05252635106444359, -0.02053857035934925, 0.01919563114643097], [-0.05240044742822647, -0.12656624615192413, -0.051676299422979355, 0.002106151543557644, 0.0036294092424213886, 0.0030965376645326614, 0.035032376646995544, 0.01765575259923935], [-0.04806097224354744, -0.1253393441438675, -0.11987081170082092, -0.08780469745397568, -0.039246685802936554, 0.008615887723863125, 0.026549076661467552, -0.005607561208307743], [-0.011176887899637222, -0.03073025681078434, -0.025029592216014862, -0.01685710996389389, -0.04079047963023186, -0.06995511054992676, -0.011855554766952991, 0.004878278821706772], [0.022710995748639107, 0.0022187656722962856, 0.0005714190192520618, 0.005870600696653128, -0.021804379299283028, -0.12494057416915894, -0.06279068440198898, 0.05603015050292015], [0.013628893531858921, -0.009617786854505539, -0.010539405047893524, -0.048408281058073044, -0.14208479225635529, -0.1523817628622055, 0.050593309104442596, 0.0701114609837532], [-0.009251241572201252, -0.03364374488592148, -0.04506116360425949, -0.03471386432647705, 0.029217597097158432, 0.1027991771697998, 0.07320724427700043, 0.011234447360038757]], \"type\": \"heatmap\"}, {\"xaxis\": \"x3\", \"yaxis\": \"y3\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.016570106148719788, -0.05301031097769737, -0.07223588973283768, -0.09112715721130371, -0.08341823518276215, -0.04347008094191551, 0.04173077642917633, 0.1103019043803215], [-0.005021908786147833, 0.015799684450030327, 0.07490313053131104, 0.08743198215961456, 0.06964350491762161, 0.005876592360436916, -0.12557636201381683, -0.12292163819074631], [0.03220881149172783, 0.12288898974657059, 0.13480079174041748, 0.056551266461610794, -0.04376186802983284, -0.15292467176914215, -0.22082187235355377, -0.07402129471302032], [0.045509953051805496, 0.143290713429451, 0.12481671571731567, -0.021541418507695198, -0.1425994336605072, -0.1914364993572235, -0.14285431802272797, -0.03332497179508209], [0.02114836871623993, 0.04532269388437271, 0.05098578706383705, 0.028943754732608795, 0.037809666246175766, -0.0007911445572972298, -0.11104768514633179, -0.08444996178150177], [0.02577134035527706, 0.004515089560300112, 0.01624993234872818, 0.041039783507585526, 0.013784602284431458, -0.016811294481158257, -0.055755890905857086, -0.07983795553445816], [0.01748250238597393, 0.03233487159013748, 0.026660384610295296, -0.02265423908829689, -0.08372875303030014, -0.059188902378082275, -0.01940283738076687, -0.018211834132671356], [-0.038658853620290756, -0.03668706491589546, -0.02060159668326378, -0.01527984905987978, 0.016882222145795822, 0.027851872146129608, 0.009353283792734146, -0.0003109730314463377]], \"type\": \"heatmap\"}, {\"xaxis\": \"x4\", \"yaxis\": \"y4\", \"colorscale\": [[0, \"#000000\"], [1, \"#FFFFFF\"]], \"showscale\": false, \"z\": [[-0.015149441547691822, -0.03594481945037842, -0.058698851615190506, -0.0729682669043541, -0.0615043081343174, -0.04179077968001366, 0.000538226799108088, 0.04396788403391838], [-0.028134040534496307, -0.05540240556001663, -0.033919207751750946, -0.008945104666054249, 0.00870728213340044, 0.0023132790811359882, -0.018643710762262344, 0.000655040261335671], [-0.014659229665994644, -0.010663664899766445, 0.0309290811419487, -0.011781168170273304, -0.04911345988512039, -0.04712396860122681, -0.039340902119874954, -0.013160605914890766], [-0.010150831192731857, 0.02476341836154461, 0.015544266439974308, -0.06974482536315918, -0.0972813218832016, -0.12111999839544296, -0.07477067410945892, -0.004275789950042963], [-0.01028510183095932, 0.020150283351540565, 0.04237167537212372, 0.03659763187170029, -0.004835459403693676, -0.06757400184869766, -0.14027459919452667, -0.06513844430446625], [-0.01215366180986166, -0.012733847834169865, 0.028817754238843918, 0.08714200556278229, 0.09594306349754333, 0.01824377290904522, -0.18620193004608154, -0.11723554879426956], [0.0025916139129549265, -0.07851575314998627, 0.022901441901922226, 0.11894527077674866, 0.06024530157446861, -0.16683439910411835, -0.2391425520181656, -0.06393174827098846], [0.0941571593284607, 0.10903573036193848, -0.01868906244635582, -0.17982622981071472, -0.26138192415237427, -0.18528465926647186, -0.05767921730875969, -0.001825208542868495]], \"type\": \"heatmap\"}], {\"showlegend\": false, \"yaxis1\": {\"domain\": [0.625, 1.0], \"showticklabels\": false, \"anchor\": \"x1\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"xaxis4\": {\"domain\": [0.55, 1.0], \"showticklabels\": false, \"anchor\": \"y4\", \"ticks\": \"\"}, \"xaxis3\": {\"domain\": [0.0, 0.45], \"showticklabels\": false, \"anchor\": \"y3\", \"ticks\": \"\"}, \"xaxis2\": {\"domain\": [0.55, 1.0], \"showticklabels\": false, \"anchor\": \"y2\", \"ticks\": \"\"}, \"xaxis1\": {\"domain\": [0.0, 0.45], \"showticklabels\": false, \"anchor\": \"y1\", \"ticks\": \"\"}, \"height\": 900, \"width\": 800, \"yaxis2\": {\"domain\": [0.625, 1.0], \"showticklabels\": false, \"anchor\": \"x2\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"yaxis3\": {\"domain\": [0.0, 0.375], \"showticklabels\": false, \"anchor\": \"x3\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"yaxis4\": {\"domain\": [0.0, 0.375], \"showticklabels\": false, \"anchor\": \"x4\", \"ticks\": \"\", \"autorange\": \"reversed\"}, \"title\": \"Weights Visualisation of Latent Matrix, W\", \"annotations\": [{\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 1\", \"y\": 1.0, \"x\": 0.225, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 2\", \"y\": 1.0, \"x\": 0.775, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 3\", \"y\": 0.375, \"x\": 0.225, \"font\": {\"size\": 16}, \"showarrow\": false}, {\"yanchor\": \"bottom\", \"xref\": \"paper\", \"xanchor\": \"center\", \"yref\": \"paper\", \"text\": \"Weight 4\", \"y\": 0.375, \"x\": 0.775, \"font\": {\"size\": 16}, \"showarrow\": false}]}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Creates 2x2 subplot of weights\n",
    "Each subplot consists of an 8x8 heatmap for the weight of each latent variable\n",
    "Input:\n",
    "    weights: final trained weights (D x K)\n",
    "'''\n",
    "def visualise_weights(weights):\n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "    \n",
    "    # Define empty figure\n",
    "    figure = tools.make_subplots(rows=2, cols=2, subplot_titles=('Weight 1', 'Weight 2', 'Weight 3', 'Weight 4'))\n",
    "    \n",
    "    # Define subplot traces\n",
    "    for i, weight in enumerate(np.transpose(weights)):\n",
    "        trace = go.Heatmap(\n",
    "            z = np.reshape(weight, (8,8)), # TODO Reverse order\n",
    "            colorscale = [[0, '#000000'], [1, '#FFFFFF']],\n",
    "            showscale = False\n",
    "        )\n",
    "        figure.append_trace(trace, i / 2 + 1, i % 2 + 1)\n",
    "        \n",
    "        figure['layout']['xaxis{}'.format(i + 1)].update(showticklabels = False, ticks = '')\n",
    "        figure['layout']['yaxis{}'.format(i + 1)].update(showticklabels = False, ticks = '', autorange='reversed')\n",
    "        \n",
    "    figure['layout'].update(\n",
    "        height = 900,\n",
    "        width = 800,\n",
    "        showlegend = False,\n",
    "        title = 'Weights Visualisation of Latent Matrix, W'\n",
    "    )\n",
    "    \n",
    "    py.iplot(figure, filename='/ECE521: A3/Q3: Factor Analysis/Q1.2_weights_viz', sharing='private')\n",
    "    return pyo.iplot(figure)\n",
    "\n",
    "visualise_weights(results_3_1_2[0]['W'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 3.3.1.3: PCA vs FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate toy dataset of 200, 3D points using the following relations:\n",
    "\n",
    "$$ x_1 = s_1 \\\\ x_2 = s_1 + 0.001 s_2 \\\\ x_3 = 10 s_3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reset random seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define variable to store toy data\n",
    "toy_data = np.array([])[np.newaxis, :].reshape(0, 3)\n",
    "\n",
    "for i in range(5000):\n",
    "    # Sample s from normal distribution\n",
    "    s = np.random.randn(3,1)\n",
    "    \n",
    "    # Define conversion matrix from s to x\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [1, 0.001, 0], \n",
    "                  [0, 0, 10]])\n",
    "    \n",
    "    toy_data = np.append(toy_data, np.transpose(np.matmul(A, s)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     4
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principle component: \n",
      "[[ -3.933e-07]\n",
      " [  4.476e-04]\n",
      " [  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Finds the largest principal component by finding the normalised eigenvector corresponding\n",
    "to the largest eigenvalue of the data covariance matrix\n",
    "'''\n",
    "def largest_component_PCA(data):\n",
    "    # Obtain covariance matrix\n",
    "    sigma = np.cov(np.transpose(data))\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    e_value, e_vector = np.linalg.eig(sigma)\n",
    "    \n",
    "    # Return the largest principle component\n",
    "    PC = e_vector[np.argmax(e_value)][:, np.newaxis]\n",
    "    \n",
    "    # Save data\n",
    "    np.save('./Results/FA/3_1_3_PCA_x{}.npy'.format(data.shape[0]), PC)\n",
    "    \n",
    "    return PC\n",
    "\n",
    "print 'Principle component: \\n{}'.format(largest_component_PCA(toy_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train using FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss:  4151\n",
      "iter: 200, train_loss:  2804\n",
      "iter: 300, train_loss:  2321\n",
      "iter: 400, train_loss:  2022\n",
      "iter: 500, train_loss:  1818\n",
      "iter: 600, train_loss:  1675\n",
      "iter: 700, train_loss:  1572\n",
      "iter: 800, train_loss:  1496\n",
      "iter: 900, train_loss:  1441\n",
      "iter: 1000, train_loss:  1401\n",
      "iter: 1100, train_loss:  1371\n",
      "iter: 1200, train_loss:  1348\n",
      "iter: 1300, train_loss:  1330\n",
      "iter: 1400, train_loss:  1321\n",
      "iter: 1500, train_loss:  1320\n",
      "iter: 1600, train_loss:  1199\n",
      "iter: 1700, train_loss:  1053\n",
      "iter: 1800, train_loss:   936\n",
      "iter: 1900, train_loss:   827\n",
      "iter: 2000, train_loss:   720\n",
      "iter: 2100, train_loss:   616\n",
      "iter: 2200, train_loss:   513\n",
      "iter: 2300, train_loss:   413\n",
      "iter: 2400, train_loss:   313\n",
      "iter: 2500, train_loss:   235\n",
      "iter: 2600, train_loss:   464\n",
      "iter: 2700, train_loss:   321\n",
      "iter: 2800, train_loss:   184\n",
      "iter: 2900, train_loss:   108\n",
      "iter: 3000, train_loss:   127\n",
      "iter: 3100, train_loss:   227\n",
      "iter: 3200, train_loss:   101\n",
      "iter: 3300, train_loss:   153\n",
      "iter: 3400, train_loss:   328\n",
      "iter: 3500, train_loss:   244\n",
      "iter: 3600, train_loss:   235\n",
      "iter: 3700, train_loss:   230\n",
      "iter: 3800, train_loss:   123\n",
      "iter: 3900, train_loss:   121\n",
      "iter: 4000, train_loss:   126\n",
      "iter: 4100, train_loss:   190\n",
      "iter: 4200, train_loss:   159\n",
      "iter: 4300, train_loss:   138\n",
      "iter: 4400, train_loss:   126\n",
      "iter: 4500, train_loss:   185\n",
      "iter: 4600, train_loss:   115\n",
      "iter: 4700, train_loss:    99\n",
      "iter: 4800, train_loss:   105\n",
      "iter: 4900, train_loss:   147\n",
      "iter: 5000, train_loss:   106\n",
      "Max iteration reached\n",
      "K:   1, duration: 16.7s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_3 = run_FA(K_list=[1], D=3, QUES_DIR='Q3.1.3', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, train_loss: 100774\n",
      "iter: 200, train_loss: 68373\n",
      "iter: 300, train_loss: 56766\n",
      "iter: 400, train_loss: 49562\n",
      "iter: 500, train_loss: 44682\n",
      "iter: 600, train_loss: 41245\n",
      "iter: 700, train_loss: 38776\n",
      "iter: 800, train_loss: 36981\n",
      "iter: 900, train_loss: 35666\n",
      "iter: 1000, train_loss: 34702\n",
      "iter: 1100, train_loss: 33990\n",
      "iter: 1200, train_loss: 33455\n",
      "iter: 1300, train_loss: 33058\n",
      "iter: 1400, train_loss: 32861\n",
      "iter: 1500, train_loss: 32838\n",
      "iter: 1600, train_loss: 32837\n",
      "iter: 1700, train_loss: 32837\n",
      "iter: 1800, train_loss: 32837\n",
      "iter: 1900, train_loss: 32837\n",
      "iter: 2000, train_loss: 32837\n",
      "iter: 2100, train_loss: 32837\n",
      "iter: 2200, train_loss: 28696\n",
      "iter: 2300, train_loss: 25324\n",
      "iter: 2400, train_loss: 22472\n",
      "iter: 2500, train_loss: 19785\n",
      "iter: 2600, train_loss: 17177\n",
      "iter: 2700, train_loss: 14624\n",
      "iter: 2800, train_loss: 12103\n",
      "iter: 2900, train_loss:  9629\n",
      "iter: 3000, train_loss:  7245\n",
      "iter: 3100, train_loss:  4967\n",
      "iter: 3200, train_loss:  3625\n",
      "iter: 3300, train_loss:  5746\n",
      "iter: 3400, train_loss:  3622\n",
      "iter: 3500, train_loss: 10435\n",
      "iter: 3600, train_loss:  2841\n",
      "iter: 3700, train_loss:  3739\n",
      "iter: 3800, train_loss:  3375\n",
      "iter: 3900, train_loss:  5760\n",
      "iter: 4000, train_loss:  5524\n",
      "iter: 4100, train_loss: 30838\n",
      "iter: 4200, train_loss:  3675\n",
      "iter: 4300, train_loss:  4807\n",
      "iter: 4400, train_loss:  3532\n",
      "iter: 4500, train_loss:  4784\n",
      "iter: 4600, train_loss:  9915\n",
      "iter: 4700, train_loss:  2980\n",
      "iter: 4800, train_loss:  6959\n",
      "iter: 4900, train_loss: 12699\n",
      "iter: 5000, train_loss:  4025\n",
      "Max iteration reached\n",
      "K:   1, duration: 35.7s\n",
      "\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results_3_1_3_x5000 = run_FA(K_list=[1], D=3, QUES_DIR='Q3.1.3', device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save FA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('./Results/FA/3_1_3.npy', results_3_1_3)\n",
    "np.save('./Results/FA/3_1_3_x5000.npy', results_3_1_3_x5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
